{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the scripts to the notebooks path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\KifiyaAIM-Course\\Week - 8&9\\Adey-Innovations-Fraud-Detection\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "print(current_dir)\n",
    "\n",
    "# Get the parent directory\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "scripts_path = os.path.join(parent_dir, 'scripts')\n",
    "\n",
    "# Insert the path to the parent directory\n",
    "sys.path.insert(0, parent_dir)\n",
    "\n",
    "# Insert the path to the Scripts directory\n",
    "sys.path.insert(0, scripts_path)\n",
    "\n",
    "# Add the parent directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to the csv files\n",
    "COUNTRY_IP_DATA = \"../data/IpAddress_to_Country.csv\"\n",
    "FRAUD_DATA = \"../data/Fraud_Data.csv\"\n",
    "CREDIT_DATA = \"../data/creditcard.csv\"\n",
    "\n",
    "# load the data into pandas dataframes\n",
    "country_ip_mapping = pd.read_csv(COUNTRY_IP_DATA)\n",
    "fraud_data = pd.read_csv(FRAUD_DATA)\n",
    "credit_data = pd.read_csv(CREDIT_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.feature_engineering import FeatureEngineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) First pass the data through the feature egineering pipeline. It contains the following processes.\n",
    "\n",
    "merege ip and fraud data -> calculate transaction velocity and frequency -> break down date features -> handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = FeatureEngineering.feature_enginering_pipeline(data=fraud_data, ip_mapping=country_ip_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from scripts.utils import load_pickle, pickle_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the root path where to export/load pickeled objects from\n",
    "EXPORT_PATH_ROOT = \"../feature_store\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First for the credit card data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "credit_card_features = [column for column in credit_data.columns if column not in [\"Class\"]]\n",
    "\n",
    "# define target \n",
    "credit_card_target = \"Class\"\n",
    "\n",
    "# save the feature into pickle file\n",
    "feature_save_path = os.path.join(EXPORT_PATH_ROOT, 'credit_features.pkl')\n",
    "pickle_object(file_path=feature_save_path, object=credit_card_features)\n",
    "\n",
    "# save the target into a pickle file\n",
    "target_save_path = os.path.join(EXPORT_PATH_ROOT, 'credit_target.pkl')\n",
    "pickle_object(file_path=target_save_path, object=credit_card_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the features and targets\n",
    "credit_X = credit_data[credit_card_features]\n",
    "credit_y = credit_data[credit_card_target]\n",
    "\n",
    "# split them into training and testing features\n",
    "credit_train, credit_test, credit_y_train, credit_y_test = train_test_split(credit_X, credit_y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define features\n",
    "fraud_features = [column for column in data.columns if column not in [\"class\", \"user_id\", \"device_id\"]]\n",
    "\n",
    "# define targets\n",
    "fraud_targets = \"class\"\n",
    "\n",
    "# save the feature into pickle file\n",
    "feature_save_path = os.path.join(EXPORT_PATH_ROOT, 'fraud_features.pkl')\n",
    "pickle_object(file_path=feature_save_path, object=credit_card_features)\n",
    "\n",
    "# save the target into a pickle file\n",
    "target_save_path = os.path.join(EXPORT_PATH_ROOT, 'fraud_target.pkl')\n",
    "pickle_object(file_path=target_save_path, object=credit_card_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the features and targets\n",
    "fraud_X = data[fraud_features]\n",
    "fraud_y = data[fraud_targets]\n",
    "\n",
    "# split them into training and testing features\n",
    "fraud_train, fraud_test, fraud_y_train, fraud_y_test = train_test_split(fraud_X, fraud_y, test_size=0.3, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Normalize the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the root path to store scalers and numerical encoders in\n",
    "ENCODERS_PATH_ROOT = \"../scalers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First normalize the credit  scoring numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the numerical features in the credit card data using the training data and save the scaler to be used during inference\n",
    "credit_train, credit_numerical_scaler = FeatureEngineering.normalize_numerical_features(data=credit_train)\n",
    "credit_test = credit_numerical_scaler.transform(X=credit_test)\n",
    "\n",
    "# save the scaler\n",
    "pickle_object(file_path=os.path.join(ENCODERS_PATH_ROOT, 'credit_scaler.pkl'), object=credit_numerical_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now normalize the fraud numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the numerical features in the fraud data using training data and save the scaler to be used during inference\n",
    "fraud_train_numeric_scaled, fraud_numerical_scaler = FeatureEngineering.normalize_numerical_features(data=fraud_train)\n",
    "scaled_columns = fraud_numerical_scaler.get_feature_names_out()\n",
    "fraud_test_numeric_scaled = fraud_numerical_scaler.transform(X=fraud_test[scaled_columns])\n",
    "\n",
    "# replace the numerical columns with the scaled ones\n",
    "fraud_train[scaled_columns] = fraud_train_numeric_scaled[scaled_columns]\n",
    "fraud_test[scaled_columns] = fraud_test_numeric_scaled\n",
    "\n",
    "# save the numeric columns that are scaled into pickle files\n",
    "pickle_object(file_path=os.path.join(EXPORT_PATH_ROOT, 'scaled_numerical_features.pkl'), object=scaled_columns)\n",
    "\n",
    "# save the scaler\n",
    "pickle_object(file_path=os.path.join(ENCODERS_PATH_ROOT, 'fraud_scaler.pkl'), object=fraud_numerical_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Now encode the categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.utils import use_label_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The credit data is skipped because all of its values are numerical data. So no need to encode it. \n",
    "Only the fraud data is going to have to categorical columns to be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode categorical features using data from training, obtain the encoders for each categorical columns\n",
    "fraud_train, categorical_encoder = FeatureEngineering.encode_categorical_data(data=fraud_train)\n",
    "\n",
    "# encode categorical features of the testing data\n",
    "for categorical_column in categorical_encoder:\n",
    "    fraud_test[categorical_column] = use_label_encoder(data=fraud_test[categorical_column], encoder=categorical_encoder[categorical_column])\n",
    "\n",
    "# save the categorical column encoders in pickle files\n",
    "pickle_object(file_path=os.path.join(EXPORT_PATH_ROOT, 'categorical_encoder.pkl'), object=categorical_encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
